[TOC]

# 5.服务：让客户端发现pod并与之通信.md

## 5.1 介绍服务

Kubernetes服务是一种一组功能相同的pod提供单一不变的接入点的资源。

### 5.1.1 创建服务

**通过kubectl expose创建服务**

[第二章示例](./2.开始使用Kubernetes和Docker#2.3.2 访问Web应用)

1.  创建一个服务对象

    ```sh
    $ kubectl expose rc kubia --type=LoadBalancer --name kubia-http
    service "kubia-http" exposed
    ```

2.  列出服务

    ```sh
    $ kubectl get services
    NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE
    kubernetes 10.3.240.1 <none> 443/TCP 34m
    kubia-http 10.3.246.185 <pending> 8080:31348/TCP 4s
    ```

    ```sh
    $ kubectl get svc
    NAME       CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE
    kubernetes 10.3.240.1    <none>        443/TCP        35m
    kubia-http 10.3.246.185  104.155.74.57 8080:31348/TCP 1m
    ```


**通过YAML描述文件来创建服务**

kubia-svc.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
```

通过kubectl create 发布文件

```sh
$ kubectl create -f kubia-svc.yaml
```

**检查新的服务**

```sh
$ kubectl get svc
NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE
kubernetes 10.111.240.1 <none> 443/TCP 30d
kubia 10.111.249.153 <none> 80/TCP 6m
```

10.111.249.153是集群内部地址，只能在集群内部被访问

**从内部集群测试服务**

-   创建一个pod,将请求发送到服务的集群IP并记录响应

-   使用ssh远程登录到其中一个Kuberctl节点上，然后使用curl命令

-   通过kubectl exec命令在一个已经存在的pod中执行curl命令

    >   1.  使用kubectl get pod命令列出所有的pod
    >
    >   2.  选择其中一个pod名称作为exec命令的执行目标
    >
    >   3.  执行命令
    >
    >       ```sh
    >       $ kubectl exec kubia-7n0gl -- curl -s http://10.111.249.153
    >       You're hit kubia-gzwli
    >       ```
    >
    >       双横杠(--)代表着kubectl命令项的结束。
    >
    >       这里横杠之后的内容是指在pod内部需要执行的命令

**配置服务上的会话亲和性**

-   多次执行同样的命令(请求)，每次调用执行应该在不同的pod上

-   特定客户端产生的所有请求每次指向同一个pod，可以设置服务的sessionAffinity属性为ClientIP(而不是默认的None)

    ```yaml
    apiVersion: v1
    kind: Service
    spec:
      sessionAffinity: ClientIP
      ...
    ```

    这种方式将会使服务代理将来自同一个client IP的所有请求转发至同一个pod上。

为什么不支持基于cookies的会话：

Kubernetes服务不是在HTTP层面上工作。服务处理TCP和UDP包，并不关心其中的载荷内容而cookie是HTTP协议的一部分。

**同一个服务暴露多个端口**

注意：在创建一个有多个端口的服务时，必须给每个端口指定名称

```yaml
apiVersion: v1
kind: Service
metadata:
  name:kubia
spec:
  ports:
  - name: http
    port: 80
    targetPort: 8080
  - name: https
    port: 443
    targetPort: 8443
  selector:
    app: kubia
```

注意：标签选择器应用于整个服务，不能对每个端口单独配置。不同pod的不端口映射关系，需要创建2个服务

**使用命名的端口**

pod端口定义：

```yaml
kind: Pod
spec:
  containers:
  - name: kubia
    ports:
    - name: http
      containerPort: 8080
    - name: https
      containerPort: 8443
```

服务中引用命名：

```yaml
apiVersion: v1
kind: Service
spec:
  ports:
  - name: http
    port: 80
    targetPort: http
  - name: https
    port: 443
    targetPort: https  
```

好处：更换端口号时无需更改服务spec

### 5.1.2 服务

kubernetes为客户提供了发现服务的IP和端口的方式

**通过环境变量发现服务**

在pod开始运行的时候，Kubernetes会初始化一系列的环境变量指向现在存在的服务

使用场景：创建的服务，要早于客户端pod的创建

通过在容器中运行env来列出所有的环境变量：

```sh
$ kubectl exec kubia-3inly env
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=kubia-3inly
KUBERNETES_SERVICE_HOST=10.111.240.1
KUBERNETES_SERVICE_PORT=443
...
KUBIA_SERVICE_HOST=10.111.249.153  # 代表kubia服务的IP
KUBIA_SERVICE_PORT=80  # 代表kubia服务的端口号
```

服务名称的横杠会被转换为下划线，并且当服务名称用作环境变量名称中的前缀时，所有的字母都是大写。如backend-datebase的服务，在环境变量中时BACKEND_DATABASE_SERVICE_HOST和BACKEND_DATABASE_SERVICE

**通过DNS发现服务**

在kube-system命名空间下名为kube-dns的pod，这个pod运行DNS服务，集群中的其它pod都被配置成使用其作为dns（kubernetes通过修改每个容器的/etc/resolv.conf文件实现）

>   pod是否使用内部的DNS服务器是根据pod中的spec的dnsPolicy属性来决定的

每个服务从内部DNS服务器中获取一个DNS条目，客户端的pod在知道服务名称的情况下，通过全限定域名(FQDN)来访问，而不是诉诸于环境变量

**通过FQDN连接服务**

FQDN: kubia.default.svc.cluster.local

>   kubia对应于服务名称
>
>   default表示服务在其中定义的名称空间
>
>   svc.cluster.local是在集群本地服务名称中使用的可配置集群域后缀

FQDN只是域名，如果访问，还需要端口号（从环境变量中获取）

如果两个pod在同一个命名空间中，可以省略svc.cluster.local后缀，甚至可以省略命名空间

**在容器中运行shell**

```sh
$ kubectl exec -it kubia-3inly bash
root@kubia-3inly:/#
```

在容器内部，可以通过以下任意方式访问kubia服务

```sh
root@kubia-3inly:/# curl http://kubia.default.svc.cluster.local
You’ve hit kubia-5asi2
root@kubia-3inly:/# curl http://kubia.default
You’ve hit kubia-3inly
root@kubia-3inly:/# curl http://kubia
You’ve hit kubia-8awf3
```

查看容器中的/etc/reliv.conf文件就能明白

```sh
root@kubia-3inly:/# cat /etc/resolv.conf
search default.svc.cluster.local svc.cluster.local cluster.local ...
```

**无法ping通服务IP**

服务的集群IP是一个虚拟IP，并且只有在和服务端口结合时才有意义

## 5.2 连接集群外部的服务

### 5.2.1 介绍服务endpoint

服务并不是和pod直接相连的，而是通过Endpoint资源

```sh
$ kubectl describe svc kubia
Name: kubia
Namespace: default
Labels: <none>
Selector: app=kubia
Type: ClusterIP
IP: 10.111.249.153
Port: <unset> 80/TCP
Endpoints: 10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080
Session Affinity: None
No events.
```

Endpoint资源是暴露一个服务的IP地址和端口的列表

```sh
$ kubectl get endpoints kubia
NAME ENDPOINTS AGE
kubia 10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080 1h
```

spec服务定义了pod选择器，但pod选择器用于构建IP和端口列表，然后存储在Endpoint资源中。

服务代理选择这些IP和端口对中的一个，并将传入连接重定向到在该位置监听的服务器。

### 5.2.2 手动配置服务的endpoint

>   如果创建了不包含pod选择器的服务，Kubernetes将不会创建Endpoint资源，也知道服务中包含哪些pod

**创建不含pod选择器的服务**

external-service.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
name: external-service
spec:
ports:
- port: 80
```

**创建Endpoint资源**

external-service-endpoints.yaml

```yaml
apiVersion: v1
kind: Endpoints
metadata:
  name: external-service
subsets:
  - addresses:
    - ip: 11.11.11.11
    - ip: 22.22.22.22
    ports:
    - port: 80 
```

### 5.2.3 为外部服务创建别名

通过其完[全限定域名(FQDN)](#5.1.2 服务)访问外部服务

**创建ExternalName类型的服务**

external-service-externalname.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: external-service
spec:
  type: ExternalName
  externalName: api.somecompany.com
  ports:
  - port: 80
```

服务创建完成后，pod可以通过extenal-service.default.svc.cluster.local域名连接外部服务

## 5.3 将服务暴露给外部客户端

可以在外部访问服务的方式:

1.  将服务的类型设置为NodePort

2.  将服务的类型设置为LoadBalance

    >   LoadBalace是NodePort类型的一种扩展——使得服务可以通过一个专用的负载均衡来访问
    >
    >   负载均衡器将流量重定向到跨所有节点的节点端口
    >
    >   客户端通过负载均衡器的IP连接到服务

3.  创建一个Ingress资源

    >   这是一个完全不同的机制，通过一个IP地址公开多个服务——运行在HTTP层(网络协议第7层)，可以提供比工作在第四层的服务更多地功能

### 5. 3.1 使用NodePort类型的服务

**创建NodePort类型的服务**

kubia-svc-nodeport.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia-nodeport
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30123
  selector:
    app: kubia
```

指定端口(nodePort)不是强制性的，忽略时会选择一个随机端口

**查看NodePort类型的服务**

```sh
$ kuberctl get svc kubia-nodeport
NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE
kubia-nodeport 10.111.254.223 <nodes> 80:30123/TCP 2m
```

![image-20200717005050318](5.%E6%9C%8D%E5%8A%A1%EF%BC%9A%E8%AE%A9%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8F%91%E7%8E%B0pod%E5%B9%B6%E4%B8%8E%E4%B9%8B%E9%80%9A%E4%BF%A1.assets/image-20200717005050318.png)

问题：这种方式通过节点IP来访问，如果节点发生故障，客户端将无法访问

### 5.3.2 通过负载均衡(Load Balancer)将服务暴露出来

负载均衡器拥有自己独一无二的可公开访问的IP地址，并将所有连接重定向到服务。

**创建LoadBalance服务**

kubia-svc-loadbalancer.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia-loadbalancer
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
```

**通过负载均衡连接服务**

```sh
$ kubectl get svc kubia-loadbalancer
NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE
kubia-loadbalancer 10.111.241.153 130.211.53.173 80:32143/TCP 1m
```

负载均衡的地址：https://130.211.53.173:80

**会话亲和性和web浏览器**
每次浏览器都会连上同一个pod：浏览器使用keep-alive连接，并通过单个连接发送所有请求，而curl命令每次都会打开一个新连接

![image-20200717232243906](5.%E6%9C%8D%E5%8A%A1%EF%BC%9A%E8%AE%A9%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8F%91%E7%8E%B0pod%E5%B9%B6%E4%B8%8E%E4%B9%8B%E9%80%9A%E4%BF%A1.assets/image-20200717232243906.png)

### 5.3.3 了解外部连接的特性

**了解并防止不必要的网络跳数**
当外部客户端通过节点端口连接到服务时（这也包括先通过负载均衡器时的情况），随机选择的pod并不一定在接受连接的同一节点上运行。可能需要额外的网络跳转才能到达pod，但这种行为并不符合期望。

阻止额外跳数：
通过在服务的spec部分中设置externalTrafficPolicy字段来完成

```yaml

	spec：
	  externalTrafficPolicy:Local
```

**记住客户IP是不记录的**
通常，当集群内的客户端连接到服务时，支持服务的pod可以获取客户端的IP地址。但是，单通过节点端口接收到新连接时，由于对数据包执行了源网络地址转换（SNAT），因此数据包的源IP将发生更改

## 5.4 通过Ingress暴露服务

**为什么使用Ingress**

-   每个Load Balancer服务都需要自己的负载均衡器，以及独有的共有IP地址
-   Ingress只需要一个公网IP就能为许多服务提供访问

![image-20200717234548307](5.%E6%9C%8D%E5%8A%A1%EF%BC%9A%E8%AE%A9%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8F%91%E7%8E%B0pod%E5%B9%B6%E4%B8%8E%E4%B9%8B%E9%80%9A%E4%BF%A1.assets/image-20200717234548307.png)



Ingress在网络栈(HTTP)的应用层操作，并且可以提供一些服务不能实现的功能。

**Ingress控制器是必不可少的**

### 5.4.1 创建Ingress资源

kubia-ingress.yaml

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  rules:
  - host: kubia.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: kubia-nodeport
          servicePort: 80
```

### 5.4.2 通过Ingress访问服务

**获取Ingress的IP地址**

```sh
$ kubectl get ingresses
NAME HOSTS ADDRESS PORTS AGE
kubia kubia.example.com 192.168.99.100 80 29m
```

确保在Ingress中配置的Host指向Ingress的IP地址

host文件

>   Linux: /etc/hosts 
>
>   Windows: C:\windows\system32\drivers\etc\hosts
>
>   添加 192.168.99.100 kubia.example.com

**通过Ingress访问pod**

```sh
$ curl http://kubia.example.com
```

**了解Ingress的工作原理**

1.  客户端首先对http://kubia.example.com执行DNS查找，DNS服务器(或本地操作系统)返回了Ingress控制器的IP
2.  客户端向Ingress控制器发送HTTP请求，并在Host头中指定kubia.example.com
3.  控制器从头部确认客户端尝试访问哪个服务，通过与该服务关联的EndPoint对象查看pod IP，并将客户端的请求转发给其中一个pod

![image-20200718000415294](5.%E6%9C%8D%E5%8A%A1%EF%BC%9A%E8%AE%A9%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8F%91%E7%8E%B0pod%E5%B9%B6%E4%B8%8E%E4%B9%8B%E9%80%9A%E4%BF%A1.assets/image-20200718000415294.png)

### 5.4.3 通过相同的Ingress暴露多个服务

**将不同的服务映射到相同主机的不同路径**

```yaml
...
- host: kubia.example.com
  http:
  paths:
  - path: /kubia
    backend:
    serviceName: kubia
    servicePort: 80
  - path: /foo
    backend:
    serviceName: bar
    servicePort: 80
```

**将不同的服务映射到不同的主机上**

```yaml
spec:
  rules:
  - host: foo.example.com
    http:
    paths:
    - path: /
      backend:
      serviceName: foo
      servicePort: 80
  - host: bar.example.com
    http:
    paths:
    - path: /
      backend:
      serviceName: bar
      servicePort: 80
```

### 5.4.4 配置Ingress处理TLS传输

**为Ingress创建TLS认证**
当客户端创建到IngressTLS连接时，控制器将终止TLS连接。

客户端和控制器之间的通信时加密的，而控制器和后端pod之间的通信则不是。

运行在pod上的应用程序不需要支持TLS

实现：将证书和私钥附加到Ingress，存储在Secret中

1.  创建私钥和证书

    >   ```sh
    >   $ openssl genrsa -out tls.key 2048
    >   $ openssl req -new -x509 -key tls.key -out tls.cert -days 360 -subj /CN=kubia.example.com
    >   ```

2.  创建Secret

    >   ```sh
    >   $ kubectl create secret tls tls-secret --cert=tls.cert --key=tls.key
    >   secret "tls-secret" created
    >   ```

3.  更新Ingress

    >   kubia-ingress-tls.yaml
    >
    >   ```yaml
    >   apiVersion: extensions/v1beta1
    >   kind: Ingress
    >   metadata:
    >     name: kubia
    >   spec:
    >     tls:
    >     - hosts: 
    >       - kubia.example.com
    >       secretName: tls-secret
    >     rules:
    >     - host: kubia.example.com
    >       http:
    >         paths:
    >         - path: /
    >           backend:
    >             serviceName: kubia-nodeport
    >             servicePort: 80
    >   ```
    >
    >   ```sh
    >   $ kubectl apply-f kubia-ingress-tls.yaml
    >   ```

4.  访问

    ```sh
    $ curl -k -v https://kubia.example.com/kubia
    * About to connect() to kubia.example.com port 443 (#0)
    ...
    * Server certificate:
    * subject: CN=kubia.example.com
    ...
    > GET /kubia HTTP/1.1
    > ...
    You've hit kubia-xueq1
    ```

## 5.5 pod就绪后发出信号

### 5.5.1 介绍就绪探针

就绪探测器会定期调用，并确定特定的pod是否接受客户端请求

**就绪探针的类型**

-   Exec探针：执行进程的地方。容器的状态由进程的推出状态代码确定
-   HTTP GET探针：先容器发送HTTP GET请求，通过响应的HTTP状态代码判断容器是否准备好
-   TCP socket探针：打开一个TCP连接到容器的特定端口。如果连接已建立，则认为已准备就绪

**了解就绪探针的操作**

1.  启动容器时，可以为kubernetes配置一个等待时间，经过等待时间后开始执行就绪检查
2.  周期性的调用探针，根据就绪探针的结果采取行动
3.  未准备就绪，从服务中删除该pod，如果pod再次准备就绪，则重新添加pod

与存活探针不一样，就绪探针的检查未通过时，容器不会被终止或重新启动

**了解就绪探针的重要性**

确保客户端只与正常的pod交互，并且永远不会知道系统存在问题

### 5.5.2 向pod添加就绪探针

kubia-rc-readinessprobe.yaml

```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
        ports:
        - name: http
          containerPort: 8080
        readinessProbe:
          exec:
            command:
            - ls
            - /var/ready
```

### 5.5.3 了解就绪探针的实际作用

**务必定义就绪探针**

**不要将停止pod的逻辑纳入就绪探针中**

## 5.6 使用headless服务来发现独立的pod

场景：客户端需要连接所有的pod

通过DNS查找，通常DNS服务器会返回服务的集群IP。当将服务spec中的clusterIP字段设置为None，则DNS服务器返回的是pod IP

### 5.6.1 创建headless服务

kubia-svc-headless.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia-headless
spec:
  clusterIP: None
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
```

### 5.6.2 通过DNS发现pod

>   kubia容器镜像不包含nslookup(或dig)二进制文件，因此无法使用它执行DNS查找

**不通过YAML文件运行Pod**

```sh
$ kubectl run dnsutils --image=tutum/dnsutils --generator=run-pod/v1 --command -- sleep infinity
pod "dnsutils" created
```

--generator=run-pod/v1 ,让kubectl直接创建pod，而不需要通过ReplicationController之类的资源来出创建

```sh
$ kubecl exec dnsutils nslookup kubia-headless
...
Name: kubia-headless.default.svc.cluster.local
Address: 10.108.1.4
Name: kubia-headless.default.svc.cluster.local
Address: 10.108.2.5
```

常规的，非headless服务返回的是服务的集群IP

```sh
$ kubectl exec dnsutils nslookup kubia
...
Name: kubia.default.svc.cluster.local
Address: 10.111.249.153
```

对于headless服务，客户也可以通过连接服务的DNS名称来连接pod，但实际客户端直接连接的pod，而不是通过服务代理。

>   headless服务，同样提供负载均衡，但是是通过DNS轮询机制，而不是通过代理

## 5.7 排除服务故障

-   确保从集群内连接到服务的集群IP，而不是从外部
-   集群IP是虚拟IP，无法ping通
-   确保就绪探针返回成功
-   通过kubectl get endpoints端点对象，确认某个容器是服务的一部分
-   FQDN访问不通时，使用集群IP尝试
-   检查是否连接到服务公开的端口，而不是目标端口
-   尝试直接连接到Pod IP
-   如pod ip访问失败，确保应用不是仅绑定大本地主机