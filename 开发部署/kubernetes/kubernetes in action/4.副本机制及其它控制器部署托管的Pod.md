# 4.副本机制和其它控制器：部署托管的pod

[TOC]



通过创建ReplicationController 或 Deployment这样的资源，接着由它们创建并管理实际的pod

## 4.1. 保持健康的pod

只要将pod调度到某个节点上，该节点的kubelet就会运行pod的容器。pod会保持运行，但如果容器的主进程崩溃，kubelet将重启容器。

场景：但有时进程没有崩溃，而应用程序停止正常工作。

解决：存活探针

### 4.1.1 介绍存活探针

kubernetes可以通过存活探针(liveness probe)检查容器是否还在运行。如果探测失败，则重启容器

>   存活探针(liveness probe) 和 就绪探针(readiness probe)不一样，适用不同的场景

kubernetes由三种探测容器的机制：

-   HTTP GET探针：对容器的IP地址(你指定的端口和路径)执行HTTPGET请求。收到探测响应且是返回响应码是2xx或3xx，则成功。返回错误响应码则探测失败，并将重启容器
-   TCP套接字探针：尝试与容器指定端口建立TCP连接
-   Exec探针：在容器中执行任意命令，并检查命令推出状态码。状态码为0则探测成功，否则失败。

#### 4.1.2 创建基于HTTP 的存活探针

 kubia-liveness-probe.yaml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: kubia-liveness
spec:
  containers:
  - image: luksa/kubia-unhealthy
    name: kubia
    livenessProbe:
      httpGet:
        path: /
        port: 8080
```

#### 4.1.3 使用存活探针

```shell
$ kubectl get po kubia-liveness
NAME            READY STATUS  RESTARTS AGE
kubia-liveness  1/1   Running 1        2m
```

RESTARTS 列显示pod已被重启一次

>获取崩溃容器的应用日志
>
>$ kubectl logs mypod --previous
>
>当前容器和之前的容器不一样，需要使用--previous查看

查看重启后的pod描述

```shell
$ kubectl describe po kubia-liveness
Name: kubia-liveness
...
Containers:
 kubia:
   Container ID: docker://480986f8
   Image: luksa/kubia-unhealthy
   Image ID: docker://sha256:2b208508
   Port:
   State: Running 
     Started: Sun, 14 May 2017 11:41:40 +0200
   Last State: Terminated 
     Reason: Error 
     Exit Code: 137 
     Started: Mon, 01 Jan 0001 00:00:00 +0000 
     Finished: Sun, 14 May 2017 11:41:38 +0200 
   Ready: True
   Restart Count: 1 
   Liveness: http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3
 ...
Events:
... Killing container with id docker://95246981:pod "kubia-liveness ..."
 container "kubia" is unhealthy, it will be killed and re-created.  
```

>   从结果中可以看到，先前的容器由于发生错误被终止，返回码是137
>
>   >   137=128+9,9是SIGKILL的信号编号，意味着这个进程被强行终止
>

### 4.1.4 配置存活探针的附加信息

liveness——存活探针的附加信息：

-   delay 延时

    没有设置延迟时，探针会在启动时立即进行探测，这样往往会失败

    设置延时

    ```yaml
    ...
    livenessProbe:
     httpGet:
      path: /
      port: 8080
     initialDelaySeconds: 15
    ```

    **务必设置一个初始延迟来说明应用程序的启动时间**

-   timeout 超时

-   period 周期

-   failure 连续失败达到此次数则重启容器(阈值)

**当容器强行终止后，会创建一个全新的容器，而不是重启原来的容器**

### 4.1.5 创建有效地存活探针

-   检查内容

    -   简易的存活探针仅仅检查服务器是否响应

    -   可以通过特定的URL地址，如/health，让应用从内部地内部运行的所有重要组件执行状态检查

        **确保/health不需要认证、且没有外部因素(如数据库连接失败)影响**

-   保持探针轻量

    不宜消耗太多计算资源

-   无需在探针中实现重试循环
    配置阈值failure即可

容器的重启由承载pod的节点上的Kubelet执行，在主服务器上运行的Kebernetes Control Plane组件不会参与此过程。

## 4.2 了解ReplicationController

背景：如果节点本身崩溃，节点上的kubelet也不存在了，随节点停止运行的pod也就无法通过kubelet进行创建

### 4.2.1 ReplicatioinController 的操作

ReplicationController是一种Kubernetes资源，

-   持续监控正在运行的pod列表

-   保证响应标签的pod数目与期望相符

    正在运行的pod太多--> 删除多余的副本

    正在运行的pod太少--> 根据pod模板创建新的副本

ReplicationController的三个主要部分

-   laber selector (标签选择器)，用于确认ReplicationController作用域中有哪些pod
-   replica count (副本个数)，指定应运行的pod数量
-   pod temple (pod模板)，用于创建新的pod

![image-20200701063428128](4.%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6%E5%8F%8A%E5%85%B6%E5%AE%83%E6%8E%A7%E5%88%B6%E5%99%A8%E9%83%A8%E7%BD%B2%E6%89%98%E7%AE%A1%E7%9A%84Pod.assets/image-20200701063428128.png)

ReplicationController 的作用

-   确保一个pod（或多个pod副本）持续运行，方法是在现有pod丢失时启动一个新pod
-   集群节点发生故障时，为故障节点上运行的所有pod创建副本
-   轻松实现pod的水平伸缩——手动/自动

pod实例永远不会重新安置到另一个节点。相反，ReplicationController会创建一个全新的pod实例，他与正在替换的实例无关

### 4.2.2 创建ReplicationController

A YAML definition of a ReplicationController: kubia-rc.yaml

```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
        ports:
        - containerPort: 8080
```

定义RC时可以不指定pod标签选择器，可以让kubernetes从pod模板中去读

### 4.2.3 使用ReplicationController

创建RC命令

```shell
$ kubectl create -f kubia-rc.yaml
replicationcontroller "kubia" created
```

查看pods

```shell
$ kubectl get pods
NAME READY STATUS RESTARTS AGE
kubia-53thy 0/1 ContainerCreating 0 2s
kubia-k0xz6 0/1 ContainerCreating 0 2s
kubia-q3vkg 0/1 ContainerCreating 0 2s
```

手动删除一个pod

```bash
$ kubectl delete pod kubia-53thy
pod "kubia-53thy" deleted
```

重新查看pods

```bash
$ kubectl get pods
NAME READY STATUS RESTARTS AGE
kubia-53thy 1/1 Terminating 0 3m
kubia-oini2 0/1 ContainerCreating 0 2s
kubia-k0xz6 1/1 Running 0 3m
kubia-q3vkg 1/1 Running 0 3m
```



查看RC

```bash
$ kubectl get rc
NAME DESIRED CURRENT READY AGE
kubia 3 3 2 3m
```

查看RC附加信息

```bash
$ kubectl describe rc kubia
Name: kubia
Namespace: default
Selector: app=kubia
Labels: app=kubia
Annotations: <none>
Replicas: 3 current / 3 desired 
Pods Status: 4 Running / 0 Waiting / 0 Succeeded / 0 Failed 
Pod Template:
 Labels: app=kubia
 Containers: ...
 Volumes: <none>
Events: 
From Type Reason Message
---- ------- ------ -------
replication-controller Normal SuccessfulCreate Created pod: kubia-53thy
replication-controller Normal SuccessfulCreate Created pod: kubia-k0xz6
replication-controller Normal SuccessfulCreate Created pod: kubia-q3vkg
replication-controller Normal SuccessfulCreate Created pod: kubia-oini2
```

### RC如何创建新的pod

RC会立即收到pod被删除的通知(API服务器允许客户端监听资源和资源列表的更改)，但不会立即进行创建替代pod的操作，而是触发检查实际的pod数量并进行操作。

![image-20200516182946240](%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6%E5%8F%8A%E5%85%B6%E5%AE%83%E6%8E%A7%E5%88%B6%E5%99%A8%E9%83%A8%E7%BD%B2%E6%89%98%E7%AE%A1%E7%9A%84Pod.assets/image-20200516182946240.png)

### 4.2.4 将pod移入或移出ReplicationController的作用域



由RC创建的pod不是绑定到一个RC，而是通过标签进行管理

但pod在metadata.ownerReferences字段，可以找到属于哪个RC。

#### 从控制器删除pod

常用场景：一个pod出现故障，先修改故障pod的标签，RC会创建一个新的pod。可以对故障pod进行分析，之后再删除

#### 修改RC的pod模板

注意：修改pod模板不会影响已存在的pod

编辑RC

```bash
$ kubectl edit rc kubia
```

通过上面命令，可以修改pod模板标签，修改副本数量

#### 水平缩放pod

-   通过编辑RC直接修改

    ```bash
    $ kubectl edit rc kubia
    ```

    修改spec.replicas的值

-   直接通过命令修改

    ```bash
    $ kubectl scale rc kubia --replicas=10
    ```

通过kubernetes进行伸缩pod，是修改期望的状态，具体修改是由kubernetes自己完成

所有这些命令都会修改RC定义的spec.replicas字段，就像通过kubctl edit进行更改一样

### 删除一个ReplicationController

```bash
# 删除RC并删除pod
$ kubectl delete rc kubia
# 删除RC但不删除pod
$ kubectl delete rc kubia --cascade=false
```

## 4.3. ReplicaSet

RC最终将被弃用

#### ReplicaSet 和 ReplicationController

-   标签匹配：
    -   RC只允许只允许匹配包含某个标签(值)的匹配
    -   RS允许匹配缺少某个标签或包含特点标签名的pod

#### 创建ReplicaSet

kubia-replicaset.yaml

```yaml
apiVersion: apps/v1beta2
kind: ReplicaSet
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
```

版本：不是v1版本API的一部分，但属于apps API组的v1beta2

matchLabers：比较简单的标签选择器，类似RC的选择器

创建

```bash
# 创建
$ kubectl creat -f kubia-replicase.yaml
# 检查
$ kubectl describe rs
NAME DESIRED CURRENT READY AGE
kubia 3 3 3 3s
# 查看详细
$ kubectl describe rs
Name: kubia
Namespace: default
Selector: app=kubia
Labels: app=kubia
Annotations: <none>
Replicas: 3 current / 3 desired
Pods Status: 3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
 Labels: app=kubia
 Containers: ...
 Volumes: <none>
Events: <none>
```

#### 使用matchExpression标签选择器

 kubia-replicaset-matchexpressions.yaml

```yaml
  ...
  selector:
    matchExpressions: 
      - key: app 
        operator: In 
        values: 
          - kubia
```

operator-运算符：

-   In：Label的值必须与其中一个指定的values匹配
-   NotIn：Label的值与任何指定的values不匹配
-   Exist: pod必须包含一个指定名称的Label（值不重要）。此时不应指定value
-   DoesNotExist：pod不得包含有指定名称的Label。value属性不得指定

#### 删除RS（与RC类似）

```bash
# 删除RS并删除pod
$ kubectl delete rs kubia
# 删除RS但不删除pod
$ kubectl delete rs kubia --cascade=false
```

## 4.4. DaemonSet

DaemonSet会在每个节点上运行一个对应的pod副本，如日志收集或资源监控等

![image-20200520060036422](%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6%E5%8F%8A%E5%85%B6%E5%AE%83%E6%8E%A7%E5%88%B6%E5%99%A8%E9%83%A8%E7%BD%B2%E6%89%98%E7%AE%A1%E7%9A%84Pod.assets/image-20200520060036422.png)

可以通过在pod模板指定nodeSelector，使得DaemonSet只在指定的node上运行。

![image-20200520060202645](%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6%E5%8F%8A%E5%85%B6%E5%AE%83%E6%8E%A7%E5%88%B6%E5%99%A8%E9%83%A8%E7%BD%B2%E6%89%98%E7%AE%A1%E7%9A%84Pod.assets/image-20200520060202645.png)

DaemonSet管理的pod完全绕过调度器，即使节点设置成不可调度的(防止pod被部署到节点上)，DaemonSet依然可以在这个节点上运行pod

ssd-monitor-daemonset.yaml

```yaml
apiVersion: apps/v1beta2
kind: DaemonSet
metadata:
  name: ssd-monitor
spec:
  selector:
    matchLabels:
      app: ssd-monitor
  template:
    metadata:
      labels:
        app: ssd-monitor
    spec:
      nodeSelector:
        disk: ssd
      containers:
      - name: main
        image: luksa/ssd-monitor
```

```bash
# 创建DeamonSet
$ kubectl create -f ssd-monitor-daemonset.yaml
daemonset "ssd-monitor" created
# 查看DS
$ kubectl get ds
NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE-SELECTOR 
ssd-monitor 0 0 0 0 0 disk=ssd
# 查看pod
$ kubectl get po
No resources found.
# 查看节点
$ kubectl get node
NAME STATUS AGE VERSION
minikube Ready 4d v1.6.0
# 在节点上添加标签
$ kubectl label node minikube disk=ssd
node "minikube" labeled
# 查看pod
$ kubectl get po
NAME READY STATUS RESTARTS AGE
ssd-monitor-hgxwq 1/1 Running 0 35s
```

## 4.5. Job：运行执行单个任务的pod

背景：ReplicationController、ReplicaSet和DamonSet会持续运行任务，永远不会达到完成状态。实际项目中，需要一个可完成的任务，即在进程终止后不应再重启

![image-20200520061536911](%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6%E5%8F%8A%E5%85%B6%E5%AE%83%E6%8E%A7%E5%88%B6%E5%99%A8%E9%83%A8%E7%BD%B2%E6%89%98%E7%AE%A1%E7%9A%84Pod.assets/image-20200520061536911.png)

#### 定义job资源

batch-job.yaml

```bash
apiVersion: batch/v1
kind: Job
metadata:
  name: batch-job
spec:
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      restartPolicy: OnFailure
      containers:
      - name: main
        image: luksa/batch-job
```

redtartPolicy: 进程终结后动作，非临时任务(RC、RS、DS)默认为Always。job不能使用默认配置，需要指定为OnFailure或Never.

```bash
# 创建Job
$ kubectl create -f exporter.yaml
# 查看jobs
$ kubectl get jobs
NAME DESIRED SUCCESSFUL AGE
batch-job 1 0 2s
# 查看pod
$ kubectl get po
NAME READY STATUS RESTARTS AGE
batch-job-28qf4 1/1 Running 0 4s
# 查看所有pod(包括已完成)
$ kubectl get po -a
NAME READY STATUS RESTARTS AGE
batch-job-28qf4 0/1 Completed 0 2m
# 查看pod日志
$ kubectl logs batch-job-28qf4
Fri Apr 29 09:58:22 UTC 2016 Batch job starting
Fri Apr 29 10:00:22 UTC 2016 Finished succesfully
# 查看jobs
$ kubectl get job
NAME DESIRED SUCCESSFUL AGE
batch-job 1 1 9m
```

#### 在job中运行多个实例

配置job为并行或串行的方式运行，设置completions和parallelism属性来完成

-   顺序运行job pod

    multi-completion-batch-job.yaml

    ```yaml
    apiVersion: batch/v1
    kind: Job
    metadata:
      name: multi-completion-batch-job
    spec:
      completions: 5 
      template:
       ......
    ```

    job会一个接一个地运行5个pod，前一个完成后再创建第二个。如果其中一个pod发生故障，工作会创建一个新的pod。不发生故障，总共创建5个pod，发生故障时时5个以上。

-   并行运行pod

    通过指定parallelism，允许同时运行多个pod

     multi-completion-parallel-batch-job.yaml

    ```yaml
    apiVersion: batch/v1
    kind: Job
    metadata:
      name: multi-completion-batch-job
    spec:
      completions: 5 
      parallelism: 2 
      template:
        ......
    ```

    ```bash
    # 查看pods
    $ kubectl get po
    NAME READY STATUS RESTARTS AGE
    multi-completion-batch-job-lmmnk 1/1 Running 0 21s
    multi-completion-batch-job-qx4nq 1/1 Running 0 21s
    # 运行后修改parallelism属性
    $ kubectl scale job multi-completion-batch-job --replicas 3
    job "multi-completion-batch-job" scaled
    ```

#### 限制Job pod完成任务的时间

通过在pod配置中设置 activeDeadlineSeconds，可以限制pod的时间。如果pod运行的时间超过此时间，系统会尝试终止pod，并将job标记为失败

>通过指定spec.backoffLimit字段，可以设置Job在失败前可以重试的次数，默认为6

## 4.6. 定期运行

Kubernetes中的cron任务通过创建CronJob资源进行配置，以cron格式指定。

 cronjob.yaml

```yaml
apiVersion: batch/v1beta1 
kind: CronJob
metadata:
  name: batch-job-every-fifteen-minutes
spec:
  schedule: "0,15,30,45 * * * *" 
  jobTemplate:
    spec:
     template: 
       metadata: 
         labels: 
           app: periodic-batch-job 
       spec: 
         restartPolicy: OnFailure 
         containers: 
         - name: main 
           image: luksa/batch-job
```

#### 指定运行截止时间

设定startingDeadlineSeconds

```yaml
apiVersion: batch/v1beta1
kind: CronJob
spec:
  schedule: "0,15,30,45 * * * *"
  startingDeadlineSeconds: 15
```

pod最迟必须在预定时间后15秒内开始运行